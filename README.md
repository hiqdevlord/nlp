# NLP - Tutorial
Repository to show how NLP can tacke real problem. Including the source code, dataset, state-of-the art in NLP

## Text Preprocessing
| Section | Sub-Section | Description | Link |
| --- | --- | --- | --- |
| Word Tokenization | Tokenize word |  | [Medium](https://medium.com/@makcedward/nlp-pipeline-word-tokenization-part-1-4b2b547e6a3) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-word_tokenization.ipynb) |
| Part of Speech | | | [Medium](https://medium.com/@makcedward/nlp-pipeline-part-of-speech-part-2-b683c90e327d) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-part_of_speech.ipynb) |
| Lemmatization | | | [Medium](https://medium.com/@makcedward/nlp-pipeline-lemmatization-part-3-4bfd7304957) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp_lemmatization.ipynb) |
| Stemming | | | [Medium](https://medium.com/@makcedward/nlp-pipeline-stemming-part-4-b60a319fd52) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-stemming.ipynb) |
| Stop Words | | | [Medium](https://medium.com/@makcedward/nlp-pipeline-stop-words-part-5-d6770df8a936) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-stop_words.ipynb) |
| Sentence Tokenization | |  | [Medium](https://medium.com/@makcedward/nlp-pipeline-sentence-tokenization-part-6-86ed55b185e6) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-sentence_tokenization.ipynb) |
| Phrase Word Recognition | |  |  |

## Text Processing
| Section | Sub-Section | Description | Link |
| --- | --- | --- | --- |
| Spell Checking | Lexicon-based |  |  |

## Infomration Retrieval 
| Section | Sub-Section | Description | Link |
| --- | --- | --- | --- |
| Pattern-based Recognition |  |  | [Medium](https://towardsdatascience.com/pattern-based-recognition-did-help-in-nlp-5c54b4e7a962)  |
| Lexicon-based Recognition |  |  | [Medium](https://towardsdatascience.com/step-out-from-regular-expression-for-feature-engineering-134e594f542c) |
| Named Entity Recognition (NER) | Pre-trained NER |  | [Medium](https://medium.com/@makcedward/named-entity-recognition-3fad3f53c91e) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-named_entity_recognition.ipynb) |
|  | Custom NER |  |  |

## Text Summarization
| Section | Sub-Section | Description | Link |
| --- | --- | --- | --- |
| Extractive Approach |  |  | [Medium](https://medium.com/@makcedward/text-summarization-extractive-approach-567fe4b85c23) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-text_summarization_extractive.ipynb) |
| Abstractive Approach |  |  |  |

## Distance Measurement
| Section | Sub-Section | Description | Link |
| --- | --- | --- | --- |
| Euclidean Distance, Cosine Similarity and Jaccard Similarity |  |  | [Medium](https://towardsdatascience.com/3-basic-distance-measurement-in-text-mining-5852becff1d7) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-3_basic_distance_measurement_in_text_mining.ipynb) |
| Edit Distance |  |  | [Medium](https://towardsdatascience.com/measure-distance-between-2-words-by-simple-calculation-a97cf4993305) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-distance-edit_distance.ipynb) |
| Word Moving Distance (WMD) |  |  | [Medium](https://towardsdatascience.com/word-distance-between-word-embeddings-cc3e9cf1d632) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-word_mover_distance.ipynb) |

## Vector Representation
| Section | Sub-Section | Description | Link |
| --- | --- | --- | --- |
| Traditional Method | Bag-of-words (BoW) | Using all words as a feature  | [Medium](https://towardsdatascience.com/3-basic-approaches-in-bag-of-words-which-are-better-than-word-embeddings-c2cbc7398016) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-bag_of_words.ipynb) |
|  | Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) |  | [Medium](https://towardsdatascience.com/2-latent-methods-for-dimension-reduction-and-topic-modeling-20ff6d7d547) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-lsa_lda.ipynb) |
| Character Level | Character Embedding | Using all character (word, number, special character) for computing word vector| [Medium](https://medium.com/@makcedward/besides-word-embedding-why-you-need-to-know-character-embedding-6096a34a3b10) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-character_embedding.ipynb) |
| Word Level | Negative Sampling and Hierarchical Softmax |  |  |
|  | Word2Vec, GloVe, fastText |  | [Medium](https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-word_embedding.ipynb) |
|  | Contextualized Word Vectors (CoVe) |  | [Medium](https://towardsdatascience.com/replacing-your-word-embeddings-by-contextualized-word-vectors-9508877ad65d) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-embeddings-word-cove.ipynb) |
|  | Embeddings from Language Models (ELMo) |  | [Medium](https://towardsdatascience.com/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-embeddings-sentence-elmo.ipynb) |
| Sentence Level | Skip-thoughts |  | [Medium](https://towardsdatascience.com/transforming-text-to-sentence-embeddings-layer-via-some-thoughts-b77bed60822c) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-skip_thoughts.ipynb) |
|  | InferSent |  | [Medium](https://towardsdatascience.com/learning-sentence-embeddings-by-natural-language-inference-a50b4661a0b8) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-embeddings-sentence-infersent.ipynb) |
| Document Level | lda2vec |  | [Medium](https://towardsdatascience.com/combing-lda-and-word-embeddings-for-topic-modeling-fe4a1315a5b4) |
|  | doc2vec | Using a unsupervised learning approach to leanr the word vectors for computing document vector | [Medium](https://towardsdatascience.com/understand-how-to-transfer-your-paragraph-to-vector-by-doc2vec-1e225ccf102) [Github](https://github.com/makcedward/nlp/blob/master/sample/embeddings/nlp-embeddings-document-doc2vec.ipynb) |

## Model Interpretation
| Section | Sub-Section | Description | Link |
| --- | --- | --- | --- |
| ELI5, LIME and Skater |  |  | [Medium](https://towardsdatascience.com/3-ways-to-interpretate-your-nlp-model-to-management-and-customer-5428bc07ce15) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-model_interpretation.ipynb) |
| SHapley Additive exPlanations (SHAP) |  |  | [Medium](https://towardsdatascience.com/interpreting-your-deep-learning-model-by-shap-e69be2b47893) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-model_interpretation_shap.ipynb) |
| Anchors |  |  | [Medium](https://towardsdatascience.com/anchor-your-model-interpretation-by-anchors-aa4ed7104032) [Github](https://github.com/makcedward/nlp/blob/master/sample/nlp-model_interpretation_anchor.ipynb) |

## Myth
| Section | Sub-Section | Description | Link |
| --- | --- | --- | --- |
| Using Deep Learning can resolve all problem? |  |  | [Medium](https://medium.com/@makcedward/how-can-use-player-name-to-predict-world-cup-with-80-accuracy-262d076544c4) [Kaggle](https://www.kaggle.com/makcedward/world-cup-prediction-with-80-accuracy-in-dl-model) |

## Source Code 
| Section | Sub-Section | Description | Link |
| --- | --- | --- | --- |
| Spellcheck |  |  | [Github](https://github.com/norvig/pytudes) |
| InferSent |  |  | [Github](https://github.com/facebookresearch/InferSent) |